<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Tobias Brandner - Portfolio</title>
    <link rel="stylesheet" type="text/css" href="style.css">
</head>
<body>
    <header>
        <h1>Tobias Brandner</h1>
        <p>Developer</p>
    </header>

    <nav>
        <ul>
            <li><a href="#about">About</a></li>
            <li><a href="#portfolio">Portfolio</a></li>
            <li><a href="#contact">Contact</a></li>
        </ul>
    </nav>

    <section id="about">
        <h2>About Me</h2>
        <p>Hi my name is Tobias Brandner. 
            I am currently studying computer science (master) at the Julius-Maximilian-University (JMU) in Würzburg, Bavaria.
            I have a special interest in algorithms, machine learning and video games.</p>
    </section>

    <section id="portfolio">
        <h2>Portfolio</h2>

        <div class="portfolio-item">
            <div class="portfolio-content">
                <div class="image-container" id="b1">
                    <img id="p1" src="images/BossNRun/IntroScreen.png" alt="Project 1">
                    <div class="image-overlay">
                        <div class="overlay-text">
                            Click to change image<br>
                            <div id="c1">Image 1 of 9</div>
                        </div>
                    </div>
                </div>                
                <div class="text-container">
                    <h3>Boss'n Run</h3>
                    <p> Movement in video games is a essential part when it comes to enjoying a game, especially while playing 3D Jump'n Runs. But what defines good movement and how can we make it feel good? <br>
                        Based on Steve Swink's book Game Feel, a GDC talk about designing a jump and Game Maker's Toolkit analysis video about Celeste, we explore which parameters define the movement of a game.
                        Therefore we designed our own 3D Jump'n Run called Boss'n Run. <br>
                        For this work I focused on exploring Game Flow in context of Boss'n Run, especially the Jump'n Run part. 
                        Therefore I teamed up with Marc Mußmann who created a obstacle generator based on different movement parameter presets inside Unreal Engine 5.
                    </p>
                    <div class="button-container">
                        <a href="https://github.com/BrandnerKasper/BossNRun" target="_blank">
                            <img src="images/buttons/gihub32.png" alt="Github Button">
                        </a>
                        <a href="https://brandnerkasper.itch.io/bossn-run" target="_blank">
                            <img src="images/buttons/itch32.png" alt="Itchio Button">
                        </a>
                    </div>                    
                </div>
            </div>
        </div>
        
        <div class="portfolio-item">
            <div class="image-container" id="b2">
                <img id="p2" src="images/CoOp/CustomCoOp.png" alt="Project 2">
                <div class="image-overlay">
                    <div class="overlay-text">
                        Click to change image<br>
                        <div id="c2">Image 1 of 2</div>
                    </div>
                </div>
            </div>
            <div class="text-container">
                <h3>Custom Context Optimization</h3>
                <p>Contrastive Language Image Pretraining (CLIP) is a comprehensive pre-trained
                    image-language model that can handle a variety of
                    different tasks by merging image and text features
                    in a common space. Using CLIP on tasks with
                    limited available data is already yielding promising
                    results, especially on classification tasks. However,
                    when used as a zero/fewshot classifier, it can be
                    very sensitive to wording, requiring good prompts
                    to achieve good results. Manual development of
                    prompts for a particular task or dataset can be very
                    time consuming. Instead of creating prompts man-
                    ually, we use Context Optimization (CoOp) to learn them by generating the con-
                    text words of the prompt through a learnable vec-
                    tor while keeping all other pre-trained parameters
                    fixed. The original CoOp used the
                    models from the original CLIP repository. We
                    want to use CoOp with the open source version of
                    CLIP called OpenCLIP2, so we can use the same
                    backbone models from the original CLIP, but also
                    additional models trained on other datasets or with
                    a new backbone. For this reason, we have adapted
                    the CoOp repository to be compatible with the new
                    models from OpenCLIP. With a Shot value of 1,
                    the same backbones of OpenCLIP perform slightly
                    worse than their CLIP counterparts and new mod-
                    els perform poorly. Increasing the shot value to 16
                    seems to fix this behavior and leads to promising re-
                    sults when combining CoOp with OpenCLIP.</p>
                    <div class="button-container">
                        <a href="https://github.com/BrandnerKasper/MP_CustomCoOp" target="_blank">
                            <img src="images/buttons/gihub32.png" alt="Github Button">
                        </a>
                    </div>
            </div>
        </div>
        <!-- Add more portfolio items as needed -->
    </section>

    <section id="contact">
        <h2>Contact Me</h2>
        <p>You can reach me at: tobias.brandner@gmx.de</p>
    </section>

    <footer>
        <p>&copy; 2023 Brandner Tobias</p>
    </footer>
    <script src="script.js"></script>
</body>
</html>
